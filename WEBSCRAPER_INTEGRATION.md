# üï∑Ô∏è WebScraper Integration f√∂r Vetenskaplig Information

## √ñversikt

WebScraper-integrationen ger LeadFinder m√∂jlighet att skrapa vetenskapligt inneh√•ll fr√•n webbplatser och analysera det med AI. Den kombinerar Playwright f√∂r browser-automation, BeautifulSoup f√∂r inneh√•llsextraktion och LangChain + LLM f√∂r intelligent analys.

## üöÄ Funktioner

### **WebScraper Service**
- **Playwright Integration**: Automatisk browser-kontroll f√∂r dynamiska webbplatser
- **BeautifulSoup**: Intelligent inneh√•llsextraktion fr√•n HTML
- **Asynkron Bearbetning**: Samtidig skrapning av flera URLs
- **Inneh√•llstyper**: St√∂d f√∂r vetenskapliga artiklar, forskarprofiler och institutioner
- **Metadata-extraktion**: Automatisk identifiering av f√∂rfattare, DOI, nyckelord, etc.

### **LangChain Analyzer**
- **Strukturerad Data**: Extraktion av vetenskaplig information i strukturerad format
- **Relevansbed√∂mning**: AI-driven bed√∂mning av inneh√•llets relevans
- **Insikter**: Automatisk generering av insikter och sammanfattningar
- **Flera Modeller**: St√∂d f√∂r b√•de Ollama (lokal) och RunPod (molnet)

## üìã Installation

### 1. Installera Beroenden

```bash
# Installera Python-paket
pip install playwright langchain langchain-community pydantic

# Installera Playwright browsers
playwright install chromium
```

### 2. Konfiguration

L√§gg till f√∂ljande i din milj√∂fil (`env.development`):

```bash
# WebScraper Configuration
WEBSCRAPER_TIMEOUT=30000
WEBSCRAPER_MAX_CONTENT_LENGTH=50000
WEBSCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# LangChain Configuration
LANGCHAIN_MODEL=mistral:latest
LANGCHAIN_CHUNK_SIZE=1000
LANGCHAIN_CHUNK_OVERLAP=200
```

## üéØ Anv√§ndning

### **Via Web Interface**

1. **Navigera till WebScraper**: G√• till `/webscraper`
2. **Ange URLs**: L√§gg till URLs till vetenskapliga artiklar, forskarprofiler eller institutioner
3. **V√§lj Inneh√•llstyp**: V√§lj mellan:
   - **Vetenskaplig artikel**: F√∂r research papers och publikationer
   - **Forskarprofil**: F√∂r forskarprofiler och CV:n
   - **Institution**: F√∂r universitet och forskningsinstitut
   - **Allm√§nt inneh√•ll**: F√∂r generell webbinneh√•ll
4. **Konfigurera AI-analys**: Aktivera LangChain-analys f√∂r strukturerad data
5. **Starta Skrapning**: Klicka p√• "Starta Skrapning"

### **Via API**

#### Skrapa Enskilda URLs
```bash
curl -X POST "http://localhost:5051/webscraper/scrape" \
  -F "urls=https://example.com/paper1" \
  -F "content_type=scientific_paper" \
  -F "research_context=epigenetics research" \
  -F "use_ai_analysis=on"
```

#### Batch-skrapning
```bash
curl -X POST "http://localhost:5051/webscraper/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://example.com/paper1", "https://example.com/paper2"],
    "content_types": ["scientific_paper", "research_profile"],
    "research_context": "epigenetics research",
    "use_ai_analysis": true
  }'
```

#### Analysera Skrapat Inneh√•ll
```bash
curl -X POST "http://localhost:5051/webscraper/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Skrapat inneh√•ll h√§r...",
    "url": "https://example.com/paper1",
    "content_type": "scientific_paper",
    "research_context": "epigenetics research"
  }'
```

## üîß Teknisk Arkitektur

### **WebScraper Service**

```python
from services.webscraper_service import webscraper_service

# Skrapa enskild URL
result = await webscraper_service.scrape_url(
    url="https://example.com/paper",
    content_type="scientific_paper"
)

# Skrapa flera URLs samtidigt
results = await webscraper_service.scrape_multiple_urls(
    urls=["url1", "url2", "url3"],
    content_types=["scientific_paper", "research_profile", "institution"]
)
```

### **LangChain Analyzer**

```python
from services.langchain_analyzer import langchain_analyzer

# Analysera vetenskaplig artikel
analysis = langchain_analyzer.analyze_scientific_paper(
    content="Artikelinneh√•ll...",
    url="https://example.com/paper",
    research_context="epigenetics research"
)

# Analysera forskarprofil
analysis = langchain_analyzer.analyze_research_profile(
    content="Profilinneh√•ll...",
    url="https://example.com/profile",
    research_context="epigenetics research"
)
```

## üìä Datastrukturer

### **ScrapedContent**
```python
@dataclass
class ScrapedContent:
    url: str                    # K√§ll-URL
    title: str                  # Sidtitel
    content: str                # Extraherat inneh√•ll
    metadata: Dict[str, Any]    # Extraherad metadata
    html: str                   # R√• HTML
    timestamp: float            # Skrapningstidpunkt
    source_type: str            # Inneh√•llstyp
```

### **ScientificPaper (LangChain)**
```python
class ScientificPaper(BaseModel):
    title: str                          # Artikeltitel
    authors: List[str]                  # F√∂rfattare
    abstract: str                       # Abstract
    doi: Optional[str]                  # DOI
    publication_date: Optional[str]     # Publiceringsdatum
    keywords: List[str]                 # Nyckelord
    institution: Optional[str]          # Institution
    funding: Optional[str]              # Finansiering
    methodology: Optional[str]          # Metodologi
    results: Optional[str]              # Resultat
    conclusions: Optional[str]          # Slutsatser
    relevance_score: int                # Relevans (1-5)
    research_areas: List[str]           # Forskningsomr√•den
    potential_collaborations: List[str] # Samarbetsm√∂jligheter
```

### **ResearchProfile (LangChain)**
```python
class ResearchProfile(BaseModel):
    name: str                           # Forskarnamn
    title: str                          # Professionell titel
    institution: str                    # Institution
    department: Optional[str]           # Avdelning
    research_interests: List[str]       # Forskningsintressen
    expertise: List[str]                # Expertisomr√•den
    publications: List[str]             # Publikationer
    contact_info: Optional[str]         # Kontaktinformation
    collaboration_potential: str        # Samarbetspotential
    relevance_score: int                # Relevans (1-5)
```

## üîÑ Integration med Lead Workshop

WebScraper kan integreras med Lead Workshop f√∂r f√∂rb√§ttrad analys:

1. **Skrapa Inneh√•ll**: Anv√§nd WebScraper f√∂r att h√§mta detaljerad information
2. **AI-analys**: Anv√§nd LangChain f√∂r strukturerad analys
3. **Lead Workshop**: Importera resultaten f√∂r vidare bearbetning

### **Exempel Integration**
```python
# I Lead Workshop
from services.webscraper_service import webscraper_service
from services.langchain_analyzer import langchain_analyzer

# Skrapa lead-inneh√•ll
scraping_result = await webscraper_service.scrape_url(lead_url, "scientific_paper")

if scraping_result.success:
    # Analysera med LangChain
    analysis = langchain_analyzer.analyze_scientific_paper(
        scraping_result.content.content,
        scraping_result.content.url,
        project_context
    )
    
    # Anv√§nd i Lead Workshop
    lead_data = {
        'title': analysis.structured_data.title,
        'authors': analysis.structured_data.authors,
        'relevance_score': analysis.structured_data.relevance_score,
        'insights': analysis.insights,
        'summary': analysis.summary
    }
```

## üõ†Ô∏è Konfiguration

### **WebScraper Inst√§llningar**

```python
# I config.py
WEBSCRAPER_CONFIG = {
    'timeout': 30000,              # 30 sekunder timeout
    'max_content_length': 50000,   # 50KB max inneh√•ll
    'user_agent': 'Mozilla/5.0...', # Browser user agent
    'retry_attempts': 3,           # Antal f√∂rs√∂k
    'retry_delay': 2               # F√∂rdr√∂jning mellan f√∂rs√∂k
}
```

### **LangChain Inst√§llningar**

```python
# I config.py
LANGCHAIN_CONFIG = {
    'model': 'mistral:latest',     # LLM-modell
    'chunk_size': 1000,            # Text-chunk storlek
    'chunk_overlap': 200,          # √ñverlapp mellan chunks
    'temperature': 0.3,            # Kreativitet (0-1)
    'max_tokens': 2000             # Max tokens per svar
}
```

## üîç Fels√∂kning

### **Vanliga Problem**

#### 1. "Playwright not available"
**L√∂sning**: Installera Playwright och browsers
```bash
pip install playwright
playwright install chromium
```

#### 2. "LangChain not available"
**L√∂sning**: Installera LangChain-paket
```bash
pip install langchain langchain-community pydantic
```

#### 3. "Browser initialization failed"
**L√∂sning**: Kontrollera system-beroenden
```bash
# Ubuntu/Debian
sudo apt-get install -y libnss3 libatk-bridge2.0-0 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxss1

# CentOS/RHEL
sudo yum install -y nss atk libdrm libxkbcommon libxcomposite libxdamage libxrandr mesa-libgbm libXScrnSaver
```

#### 4. "Content extraction failed"
**L√∂sning**: Kontrollera URL-tillg√§nglighet och inneh√•llstyp
- Verifiera att URL:en √§r tillg√§nglig
- Kontrollera att inneh√•llstypen matchar sidan
- √ñka timeout-v√§rdet om sidan laddar l√•ngsamt

### **Debugging**

Aktivera detaljerad loggning:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Kontrollera service-status:
```bash
curl http://localhost:5051/webscraper/status
```

Testa skrapning:
```bash
curl http://localhost:5051/webscraper/test
```

## üìà Prestanda

### **Optimering**

1. **Samtidig Skrapning**: Anv√§nd `scrape_multiple_urls` f√∂r batch-bearbetning
2. **Inneh√•llsfiltrering**: Begr√§nsa inneh√•llsl√§ngd med `max_content_length`
3. **Caching**: Implementera caching f√∂r √•teranv√§ndning av skrapat inneh√•ll
4. **Rate Limiting**: L√§gg till f√∂rdr√∂jningar mellan requests

### **Prestanda-m√§tningar**

- **Skrapning**: 2-10 sekunder per URL (beroende p√• sidstorlek)
- **AI-analys**: 5-30 sekunder per inneh√•ll (beroende p√• modell)
- **Samtidig kapacitet**: 5-10 URLs samtidigt (beroende p√• systemresurser)

## üîí S√§kerhet

### **Best Practices**

1. **Rate Limiting**: Begr√§nsa antal requests per tidsenhet
2. **User Agent**: Anv√§nd realistisk user agent
3. **Respect Robots.txt**: Kontrollera robots.txt innan skrapning
4. **Error Handling**: Hantera fel gracefully
5. **Content Validation**: Validera skrapat inneh√•ll

### **S√§kerhetskonfiguration**

```python
WEBSCRAPER_SECURITY = {
    'respect_robots_txt': True,    # Respektera robots.txt
    'rate_limit': 10,              # Max requests per minut
    'user_agent': 'LeadFinder Bot', # Identifierbar user agent
    'timeout': 30000,              # Timeout f√∂r requests
    'max_redirects': 5             # Max omdirigeringar
}
```

## üöÄ Framtida Utveckling

### **Planerade Funktioner**

1. **Intelligent Skrapning**: AI-driven identifiering av relevant inneh√•ll
2. **Schema.org Support**: Extraktion av strukturerad data fr√•n schema.org
3. **PDF Support**: Skrapning av PDF-dokument
4. **Social Media Integration**: Skrapning fr√•n sociala medier
5. **Real-time Monitoring**: √ñvervakning av webbplatser f√∂r nya inneh√•ll
6. **Advanced Caching**: Smart caching med versioning
7. **Distributed Scraping**: Distribuerad skrapning f√∂r h√∂g prestanda

### **API-ut√∂kningar**

- **Webhook Support**: Notifieringar vid nya inneh√•ll
- **Scheduled Scraping**: Schemalagd skrapning
- **Content Monitoring**: √ñvervakning av specifika webbplatser
- **Export Formats**: St√∂d f√∂r fler export-format

## üìö Exempel

### **Komplett Exempel**

```python
import asyncio
from services.webscraper_service import webscraper_service
from services.langchain_analyzer import langchain_analyzer

async def analyze_scientific_papers():
    # URLs till vetenskapliga artiklar
    urls = [
        "https://example.com/paper1",
        "https://example.com/paper2",
        "https://example.com/paper3"
    ]
    
    # Skrapa inneh√•ll
    scraping_results = await webscraper_service.scrape_multiple_urls(
        urls, 
        ["scientific_paper"] * len(urls)
    )
    
    # Analysera med LangChain
    for result in scraping_results:
        if result.success:
            analysis = langchain_analyzer.analyze_scientific_paper(
                result.content.content,
                result.content.url,
                "epigenetics research"
            )
            
            if analysis.success:
                print(f"Title: {analysis.structured_data.title}")
                print(f"Authors: {analysis.structured_data.authors}")
                print(f"Relevance: {analysis.structured_data.relevance_score}/5")
                print(f"Summary: {analysis.summary}")
                print("---")

# K√∂r analys
asyncio.run(analyze_scientific_papers())
```

Detta ger dig en komplett l√∂sning f√∂r att skrapa och analysera vetenskapligt inneh√•ll med modern AI-teknik! 